{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import psycopg\n",
    "import uuid\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"USER_DB\")\n",
    "password = quote(os.getenv(\"PASSWORD_DB\"))\n",
    "host = os.getenv(\"HOST_DB\")\n",
    "database = os.getenv(\"DATABASE_DB\")\n",
    "port = os.getenv(\"PORT_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper for extracting question prefixes using LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "# Pydantic model to parse the response\n",
    "class ExtractedData(BaseModel):\n",
    "    questions_prefix: list[str]\n",
    "\n",
    "async def extract_question_prefixes(pages):\n",
    "    page_content = \"\\n\".join([page.page_content for page in pages])\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Extract question prefixes.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Here is the text:\\n\\n{page_content}\"},\n",
    "        ],\n",
    "        response_format=ExtractedData,\n",
    "    )\n",
    "\n",
    "    extracted_data = completion.choices[0].message.parsed\n",
    "    \n",
    "    questions_prefix = [question_prefix.strip() for question_prefix in extracted_data.questions_prefix]\n",
    "    \n",
    "    return questions_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursively load and process PDF files in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data\"\n",
    "\n",
    "async def process_pdfs(root_dir):\n",
    "    all_pages = []\n",
    "    prefix_by_docs = {}\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(dirpath, file)\n",
    "                print(f\"Loading: {file_path}\")\n",
    "\n",
    "                # Load and split PDF into pages\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                pages = loader.load_and_split()\n",
    "                \n",
    "                # Extract question prefixes\n",
    "                questions_prefix = await extract_question_prefixes(pages)\n",
    "                \n",
    "                prefix_by_docs[file_path] = questions_prefix\n",
    "                \n",
    "                # Add structured metadata from folder structure\n",
    "                for page in pages:\n",
    "                    page.metadata['contest'] = file_path.split(\"\\\\\")[1] \n",
    "                    page.metadata['year'] = file_path.split(\"\\\\\")[2] \n",
    "                    page.metadata['levels'] = file_path.split(\"\\\\\")[3] \n",
    "                    page.metadata['subjects'] = file_path.split(\"\\\\\")[4] if file_path.split(\"\\\\\")[4] in [\"Informatique\", \"Mathématiques\"] else \"\"\n",
    "                    page.metadata['type'] = file_path.split(\"\\\\\")[5] \n",
    "                    page.metadata['exam'] = file_path.split(\"\\\\\")[6].rstrip(\".pdf\")\n",
    "                    \n",
    "                    print(file_path, page.metadata)\n",
    "                    \n",
    "                all_pages.extend(pages)\n",
    "\n",
    "    return all_pages, prefix_by_docs            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages, prefix_by_docs = await process_pdfs(\"data\")\n",
    "\n",
    "# all_pages = [\n",
    "#   Document(\n",
    "#     page_content=\"lorem ipsum dolor sit amet, consectetur adipiscing elit...\",\n",
    "#     metadata={\n",
    "#       'source': 'data/X-ENS/2022/MP/Mathématiques/écrit/MATHEMATIQUES A.pdf',\n",
    "#       'contest': 'X-ENS',\n",
    "#       'year': '2022',\n",
    "#       'levels': 'MP',\n",
    "#       'subjects': 'Mathématiques',\n",
    "#       'type': 'écrit',\n",
    "#       'exam': 'MATHEMATIQUES A'\n",
    "#     }\n",
    "#   ),\n",
    "#   Document(\n",
    "#     page_content=\"lorem ipsum dolor sit amet, consectetur adipiscing elit...\",\n",
    "#     metadata={...}\n",
    "#   ),\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# prefix_by_docs = {\n",
    "#   'data/X-ENS/2022/MP/Mathématiques/écrit/MATHEMATIQUES A.pdf': [\n",
    "#     \"Question 1\",\n",
    "#     \"Question 2\",\n",
    "#     \"Question 3\",\n",
    "#     etc.,\n",
    "#   ],\n",
    "#   'data/Mines/2023/PC/Informatique/écrit/INFORMATIQUE B.pdf': [\n",
    "#     \"Exercice 1\",\n",
    "#     \"Exercice 2\",\n",
    "#     \"Problème\",\n",
    "#     etc.,\n",
    "#   ]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_pages[0])\n",
    "# all_pages[0].page_content\n",
    "# all_pages[0].metadata\n",
    "print(prefix_by_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PostgreSQL connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "    with psycopg.connect(conninfo=connection) as conn:\n",
    "        print(\"Connection successful!\")\n",
    "except psycopg.OperationalError as e:\n",
    "    print(f\"OperationalError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the PGVector vector store with Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "collection_name = \"rapports_jurys\"\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\") \n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and split text into chunks (max 4096 tokens per chunk for LLaMA 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], \n",
    "    chunk_size=1200, \n",
    "    chunk_overlap=300)\n",
    "\n",
    "chunks = text_splitter.split_documents(all_pages)\n",
    "\n",
    "# Add extracted prefixes to chunk metadata if found in content\n",
    "for chunk in chunks:\n",
    "    questions_prefix = prefix_by_docs[chunk.metadata[\"source\"]]\n",
    "    questions_prefix_filtered =  \" | \".join(question_prefix for question_prefix in questions_prefix if question_prefix in chunk.page_content)\n",
    "    chunk.metadata[\"questions_prefix\"] = questions_prefix_filtered\n",
    "    \n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert processed documents into vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, chunk in enumerate(chunks):\n",
    "    print(num)\n",
    "    chunk.metadata[\"id\"] = f\"{chunk.metadata['source']}_{uuid.uuid4()}\"\n",
    "    vector_store.add_documents([chunk], ids=[chunk.metadata[\"id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample PostgreSQL query to test retrieval of vectors with specific metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to check in psql\n",
    "\n",
    "# SELECT *\n",
    "# FROM public.langchain_pg_embedding\n",
    "# WHERE cmetadata->>'exam' = 'MATHEMATIQUES A'\n",
    "#   AND cmetadata->>'type' = 'écrit'\n",
    "#   AND cmetadata->>'year' = '2022'\n",
    "#   AND cmetadata->>'levels' = 'MP'\n",
    "#   AND cmetadata->>'contest' = 'X-ENS'\n",
    "#   AND cmetadata->>'subjects' = 'Mathématiques'\n",
    "#   AND cmetadata->'questions_prefix' @> '[\"12\"]';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapportsjurys-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
