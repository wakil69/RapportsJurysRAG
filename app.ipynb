{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG for French Competitive Exam Reports (Rapports de Jury)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from urllib.parse import quote\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "from time import sleep\n",
    "from itertools import chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"USER_DB\")\n",
    "password = quote(os.getenv(\"PASSWORD_DB\"))\n",
    "host = os.getenv(\"HOST_DB\")\n",
    "database = os.getenv(\"DATABASE_DB\")\n",
    "port = os.getenv(\"PORT_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup (change model as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\")  # ChatOpenAI()  # do not forget to replace the embeddings and to recreate the db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Connection to PostgreSQL (PGVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "    with psycopg.connect(conninfo=connection) as conn:\n",
    "        print(\"Connection successful!\")\n",
    "except psycopg.OperationalError as e:\n",
    "    print(f\"OperationalError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "collection_name = \"rapports_jurys\"\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\") \n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Structured Metadata Schema for Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    exam: Optional[Literal[\"MATHEMATIQUES A\", \"MATHEMATIQUES B\", \"MATHEMATIQUES C\",\"MATHEMATIQUES D\", \"MATHEMATIQUES\", \"MATHEMATIQUES 1\", \"MATHEMATIQUES 2\", \"INFORMATIQUE\", \"INFORMATIQUE A\", \"INFORMATIQUE B\", \"INFORMATIQUE C\"]] = Field(\n",
    "        None, description=\"Le nom de l'√©preuve (par exemple, MATHEMATIQUES, MATHEMATIQUES A, INFORMATIQUE C, INFORMATIQUE, etc.) en majuscule. Les valeurs possibles sont MATHEMATIQUES A, MATHEMATIQUES B, MATHEMATIQUES C, MATHEMATIQUES D, MATHEMATIQUES, MATHEMATIQUES 1, MATHEMATIQUES 2, INFORMATIQUE, INFORMATIQUE A, INFORMATIQUE B, INFORMATIQUE C, ou None.\"\n",
    "    )\n",
    "    type: Optional[Literal[\"√©crit\", \"oral\"]] = Field(\n",
    "        None, description=\"Le type de l'examen. Les valeurs possibles sont '√©crit', 'oral', ou None.\"\n",
    "    )\n",
    "    contest: Optional[Literal[\"X-ENS\", \"Concours Centrale-Sup√©lec\"]] = Field(\n",
    "        None, description=\"\"\"Le concours (par exemple, X-ENS). Les valeurs possibles q sont 'X-ENS', 'Concours Centrale-Sup√©lec', ou None\n",
    "        X-ENS a plusieurs synonymes comme Polytechnique, X, ENS etc.\n",
    "        Concours Centrale-Sup√©lec a √©galement plusieurs synonymes comme centrale, supelec, etc.\n",
    "        Choisis le plus probable entre les valeurs possibles qui sont 'X-ENS', 'Concours Centrale-Sup√©lec', ou None. \n",
    "        \"\"\"\n",
    "    )\n",
    "    year: Optional[str] = Field(\n",
    "        None, description=\"L'ann√©e de la question (par exemple, 2022).\"\n",
    "    )\n",
    "    levels: Optional[Literal[\"MP\", \"PC\", \"MPI\", \"PSI\"]] = Field(\n",
    "        None, description=\"Les niveaux ou pr√©fixes de la question (par exemple, MP, PC). Les valeurs possibles sont 'MP', 'PC', 'MPI', 'PSI' ou None.\"\n",
    "    )\n",
    "    subjects: Optional[Literal[\"Math√©matiques\", \"Informatique\"]] = Field(\n",
    "        None, description=\"Le sujet li√© √† la question. Les valeurs possibles sont 'Math√©matiques', 'Informatique' ou None.\"\n",
    "    )\n",
    "    questions_prefix: Optional[str] = Field(\n",
    "        None, description=\"Le pr√©fixe de la question (par exemple, 10, 1a., etc.). Autorise None.\"\n",
    "    )\n",
    "    \n",
    "output_parser = PydanticOutputParser(pydantic_object=Question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Question into Structured Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StructuredQuestion(llm, question: str, output_parser=output_parser, max_retries=10): \n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                Tu es un agent qui structure les donn√©es sous format JSON. Structure moi la question donn√©e si possible avec ces champs :\n",
    "                - exam : Le nom de l'√©preuve (par exemple, MATHEMATIQUES A) sans accent.\n",
    "                - type : Le type de l'examen (√©crit ou oral).\n",
    "                - contest : Le concours (par exemple, X-ENS).\n",
    "                - year : L'ann√©e de la question (par exemple, 2022).\n",
    "                - levels : Les niveaux ou pr√©fixes de la question (par exemple, MP, PC).\n",
    "                - subjects : Le sujet li√© √† la question (par exemple, Math√©matiques). \n",
    "                - questions_prefix : Le pr√©fixe de la question (par exemple, 1a, 2a.).\n",
    "\n",
    "                Contrainte:\n",
    "                - Pour chaque champ, retourne une chaine de charact√®res, pas de liste !\n",
    "                \n",
    "                Encadre le r√©sultat avec des balises `json`.\\n{format_instructions}\n",
    "                \"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Voici la question: {question}\"\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(format_instructions=output_parser.get_format_instructions())\n",
    "    \n",
    "    chain = prompt | llm | output_parser\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            res = chain.invoke({\"question\": question})\n",
    "\n",
    "            if res and any(value is not None for value in res.model_dump().values()):\n",
    "                if res.type is None:\n",
    "                    res.exam = None\n",
    "                    res.questions_prefix = []\n",
    "                return res \n",
    "\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed: All fields are None.\")\n",
    "            if attempt < max_retries - 1:\n",
    "                sleep(1)  # small delay between retries\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                sleep(1) \n",
    "                \n",
    "    print(\"All retry attempts failed.\")\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the Question Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Peux-tu me dire ce que dit le rapport de jury pour l'√©preuve √©crite de maths A pour la question 12 en fili√®re MP pour le concours Polytechnique en 2022 ?\"\n",
    "\n",
    "# question_metadata = StructuredQuestion(llm, question, output_parser)\n",
    "\n",
    "# question_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-question Generation to Improve Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubQuestions(BaseModel):\n",
    "    question_one: str = Field(..., description=\"La premi√®re sous-question g√©n√©r√©e √† partir de la question principale.\")\n",
    "    question_two: str = Field(..., description=\"La deuxi√®me sous-question g√©n√©r√©e √† partir de la question principale.\")\n",
    "    question_three: str = Field(..., description=\"La troisi√®me sous-question g√©n√©r√©e √† partir de la question principale.\")    \n",
    "    \n",
    "output_parser_sub_questions = PydanticOutputParser(pydantic_object=SubQuestions)\n",
    "\n",
    "def SubQuestionsGeneration(llm, question: str, question_metadata: Question, output_parser_sub_questions=output_parser_sub_questions): \n",
    "    metadata = (\n",
    "        f\"Examen : {question_metadata.exam or 'Non sp√©cifi√©'}\\n\"\n",
    "        f\"Type : {question_metadata.type or 'Non sp√©cifi√©'}\\n\"\n",
    "        f\"Concours : {question_metadata.contest or 'Non sp√©cifi√©'}\\n\"\n",
    "        f\"Ann√©e : {question_metadata.year or 'Non sp√©cifi√©e'}\\n\"\n",
    "        f\"Niveau : {question_metadata.levels or 'Non sp√©cifi√©'}\\n\"\n",
    "        f\"Sujet : {question_metadata.subjects or 'Non sp√©cifi√©'}\\n\"\n",
    "        f\"Pr√©fixe de la question : {question_metadata.questions_prefix or 'Non sp√©cifi√©'}\"\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                Ta t√¢che est de g√©n√©rer trois variantes de la question pos√©e par l'utilisateur et des m√©tadonn√©es suivantes:\n",
    "                {metadata}\n",
    "                \n",
    "                Ces reformulations doivent permettre de r√©cup√©rer efficacement des documents pertinents dans une base de donn√©es vectorielle tout en compensant les limites de la recherche par similarit√©.\n",
    "\n",
    "                Le but est de g√©n√©rer des questions pertinentes en lien avec les rapports de jury \n",
    "                des sujets de concours des classes pr√©paratoires scientifiques.\n",
    "                \n",
    "                Retourne le r√©sultat sous forme de JSON avec les champs suivants :\n",
    "                - question_one : La premi√®re sous-question.\n",
    "                - question_two : La deuxi√®me sous-question.\n",
    "                - question_three : La troisi√®me sous-question.\n",
    "                \n",
    "                Encadre la r√©ponse avec des balises `json`.\\n{format_instructions}\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Voici la question: {question}\"\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(format_instructions=output_parser_sub_questions.get_format_instructions())\n",
    "    \n",
    "    chain = prompt | llm | output_parser_sub_questions\n",
    "                \n",
    "    res = chain.invoke({\"question\": question, \"metadata\": metadata})\n",
    "        \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Sub-question Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subquestions = SubQuestionsGeneration(llm, question, question_metadata)\n",
    "\n",
    "# subquestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîô Step-Back Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepBackQuestion(BaseModel):\n",
    "    step_back_question: str = Field(..., description=\"Une question plus g√©n√©rale g√©n√©r√©e √† partir de la question principale.\")\n",
    "    \n",
    "output_parser_step_back = PydanticOutputParser(pydantic_object=StepBackQuestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def StepBackQuestionGeneration(llm, question: str, output_parser_step_back=output_parser_step_back):     \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                Ta t√¢che est de g√©n√©rer une question plus g√©n√©rale par rapport √† la question donn√©e par l'utilisateur.\n",
    "                \n",
    "                La question g√©n√©r√©e doit fournir un contexte plus large √† la question initiale tout en restant pertinente pour les rapports de jury des sujets de concours des classes pr√©paratoires scientifiques.\n",
    "                \n",
    "                Encadre la r√©ponse avec des balises `json`.\\n{format_instructions}\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Voici la question: {question}\"\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(format_instructions=output_parser_step_back.get_format_instructions())\n",
    "    \n",
    "    chain = prompt | llm | output_parser_step_back\n",
    "                \n",
    "    res = chain.invoke({\"question\": question})\n",
    "        \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the step back question parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_back = StepBackQuestionGeneration(llm, question, question_metadata)\n",
    "\n",
    "# step_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reciprocal Rank Fusion for Multi-query Retrieval\n",
    "\n",
    "Link to the method : https://medium.com/@devalshah1619/mathematical-intuition-behind-reciprocal-rank-fusion-rrf-explained-in-2-mins-002df0cc5e2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):            \n",
    "            page_content = doc[0].page_content\n",
    "            if page_content not in fused_scores:\n",
    "                fused_scores[page_content] = 0\n",
    "            fused_scores[page_content] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (page_content, score)\n",
    "        for page_content, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Generator for Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRAG(question_metadata):\n",
    "    filter = {}\n",
    "\n",
    "    for key, value in question_metadata.model_dump().items():\n",
    "        if value is None:\n",
    "            continue\n",
    "\n",
    "        # if key == \"questions_prefix\":\n",
    "        #     filter[key] = {\"$in\": [value]}\n",
    "        if key == \"levels\" or key == \"questions_prefix\":\n",
    "            filter[key] = {\"$ilike\": f\"%{value}%\"}\n",
    "        else:\n",
    "            filter[key] = {\"$eq\": value}\n",
    "    return filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RapportsJurysRAG:\n",
    "    def __init__(self, vector_store, question):\n",
    "        self.llm = OllamaLLM(model=\"llama3.1\")\n",
    "        self.vector_store = vector_store\n",
    "        self.question = question\n",
    "        self.multiQuerySubQuestions = False # add variants questions\n",
    "        self.QueryStepBack = False # add an overview context\n",
    "        self.multiQueryRAGFusion = False # use rrf ranking\n",
    "        self.nbDocs = 5 # nb docs in the context\n",
    "        self.similarity_score_threshold = 0.3 # 0: no similarity, 1: full similarity\n",
    "        \n",
    "    def retriever(self):\n",
    "        \n",
    "        try:\n",
    "            question_metadata = StructuredQuestion(self.llm, self.question)\n",
    "                                    \n",
    "            if (self.multiQuerySubQuestions):\n",
    "                \n",
    "                sub_questions_generation = SubQuestionsGeneration(self.llm, self.question, question_metadata)\n",
    "                sub_questions = dict(sub_questions_generation).values()\n",
    "                results = []\n",
    "                                \n",
    "                for sub_question in sub_questions:\n",
    "                    \n",
    "                    sub_question_metadata = StructuredQuestion(self.llm, sub_question)\n",
    "                    filter = filterRAG(sub_question_metadata)\n",
    "                    results.append(self.vector_store.similarity_search_with_relevance_scores(sub_question, k=self.nbDocs, filter=filter))\n",
    "                                    \n",
    "                if (self.multiQueryRAGFusion):\n",
    "                \n",
    "                    retrieval_results = reciprocal_rank_fusion(results)\n",
    "                    contexte = \"\\n\".join([page_content for page_content, _ in retrieval_results[:self.nbDocs]])  \n",
    "                    return contexte, question_metadata\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    flat_results = list(chain.from_iterable(results))\n",
    "                    unique_results = {doc[0].metadata['id']: doc for doc in flat_results}\n",
    "                    sorted_results = sorted(unique_results.values(), key=lambda x: x[1], reverse=True)\n",
    "                    contexte = \"\\n\".join([doc[0].page_content for doc in sorted_results[:self.nbDocs]])                      \n",
    "                    return contexte, question_metadata\n",
    "                \n",
    "            elif (self.QueryStepBack):\n",
    "                \n",
    "                step_back_question_output = StepBackQuestionGeneration(self.llm, self.question)   \n",
    "                step_back_question = step_back_question_output.step_back_question            \n",
    "                step_back_question_metadata = StructuredQuestion(self.llm, step_back_question)                   \n",
    "                filter_question = filterRAG(question_metadata)\n",
    "                filter_step_back = filterRAG(step_back_question_metadata)\n",
    "                results_question = self.vector_store.similarity_search_with_relevance_scores(self.question, k=5, filter=filter_question)\n",
    "                results_step_back_question = self.vector_store.similarity_search_with_relevance_scores(step_back_question, k=5, filter=filter_step_back)                \n",
    "                contexte = \"\\n\".join([result[0].page_content for result in results_question[:self.nbDocs] + results_step_back_question[:self.nbDocs]])\n",
    "                return contexte, question_metadata\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                filter = filterRAG(question_metadata)\n",
    "                results = self.vector_store.similarity_search_with_relevance_scores(self.question, k=self.nbDocs, filter=filter) #, score_threshold=self.similarity_score_threshold\n",
    "                contexte = \"\\n\".join([result[0].page_content for result in results])  \n",
    "                return contexte, question_metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return \"\", {}\n",
    "        \n",
    "        \n",
    "    def create_chain(self):\n",
    "        template = \"\"\"\n",
    "        Tu es un agent IA sur les rapports de jury concernant les sujets de concours en classe pr√©paratoire scientifique. \n",
    "        R√©ponds √† la question de l'utilisateur en utilisant le contexte. La metadata est cens√© t'aider √† discerner les parties pertinentes du contexte pour g√©n√©rer la r√©ponse. \n",
    "        Si tu ne connais pas la r√©ponse, dis simplement que tu ne sais pas.\n",
    "        Question: {question} \n",
    "        Metadata: {metadata}\n",
    "        Contexte: {contexte} \n",
    "        \"\"\" \n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "            \n",
    "        chain = (\n",
    "             prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        return chain\n",
    "\n",
    "    def answer(self):\n",
    "        contexte, question_metadata = self.retriever()\n",
    "        \n",
    "        input_data = {\n",
    "            \"question\": self.question,\n",
    "            \"metadata\": question_metadata,\n",
    "            \"contexte\": contexte\n",
    "        }\n",
    "        \n",
    "        chain = self.create_chain()\n",
    "\n",
    "        res = chain.invoke(input=input_data)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Peux-tu me dire ce que dit le rapport de jury pour l'√©preuve √©crite de maths A pour la question 12 en fili√®re MP pour le concours Polytechnique en 2022 ?\"\n",
    "\n",
    "rapports_jurys = RapportsJurysRAG(vector_store, question)\n",
    "\n",
    "answer = rapports_jurys.answer()\n",
    "\n",
    "print(\"R√©ponse: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapportsjurys-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
