{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG for French Competitive Exam Reports (Rapports de Jury)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from urllib.parse import quote\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "from time import sleep\n",
    "from itertools import chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"USER_DB\")\n",
    "password = quote(os.getenv(\"PASSWORD_DB\"))\n",
    "host = os.getenv(\"HOST_DB\")\n",
    "database = os.getenv(\"DATABASE_DB\")\n",
    "port = os.getenv(\"PORT_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup (change model as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\")  # ChatOpenAI()  # do not forget to replace the embeddings and to recreate the db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Connection to PostgreSQL (PGVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "    with psycopg.connect(conninfo=connection) as conn:\n",
    "        print(\"Connection successful!\")\n",
    "except psycopg.OperationalError as e:\n",
    "    print(f\"OperationalError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "collection_name = \"rapports_jurys\"\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\") \n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Structured Metadata Schema for Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    exam: Optional[Literal[\"MATHEMATIQUES A\", \"MATHEMATIQUES B\", \"MATHEMATIQUES C\",\"MATHEMATIQUES D\", \"MATHEMATIQUES\", \"MATHEMATIQUES 1\", \"MATHEMATIQUES 2\", \"INFORMATIQUE\", \"INFORMATIQUE A\", \"INFORMATIQUE B\", \"INFORMATIQUE C\"]] = Field(\n",
    "        None, description=\"Le nom de l'épreuve (par exemple, MATHEMATIQUES, MATHEMATIQUES A, INFORMATIQUE C, INFORMATIQUE, etc.) en majuscule. Les valeurs possibles sont MATHEMATIQUES A, MATHEMATIQUES B, MATHEMATIQUES C, MATHEMATIQUES D, MATHEMATIQUES, MATHEMATIQUES 1, MATHEMATIQUES 2, INFORMATIQUE, INFORMATIQUE A, INFORMATIQUE B, INFORMATIQUE C, ou None.\"\n",
    "    )\n",
    "    type: Optional[Literal[\"écrit\", \"oral\"]] = Field(\n",
    "        None, description=\"Le type de l'examen. Les valeurs possibles sont 'écrit', 'oral', ou None.\"\n",
    "    )\n",
    "    contest: Optional[Literal[\"X-ENS\", \"Concours Centrale-Supélec\"]] = Field(\n",
    "        None, description=\"\"\"Le concours (par exemple, X-ENS). Les valeurs possibles q sont 'X-ENS', 'Concours Centrale-Supélec', ou None\n",
    "        X-ENS a plusieurs synonymes comme Polytechnique, X, ENS etc.\n",
    "        Concours Centrale-Supélec a également plusieurs synonymes comme centrale, supelec, etc.\n",
    "        Choisis le plus probable entre les valeurs possibles qui sont 'X-ENS', 'Concours Centrale-Supélec', ou None. \n",
    "        \"\"\"\n",
    "    )\n",
    "    year: Optional[str] = Field(\n",
    "        None, description=\"L'année de la question (par exemple, 2022).\"\n",
    "    )\n",
    "    levels: Optional[Literal[\"MP\", \"PC\", \"MPI\", \"PSI\"]] = Field(\n",
    "        None, description=\"Les niveaux ou préfixes de la question (par exemple, MP, PC). Les valeurs possibles sont 'MP', 'PC', 'MPI', 'PSI' ou None.\"\n",
    "    )\n",
    "    subjects: Optional[Literal[\"Mathématiques\", \"Informatique\"]] = Field(\n",
    "        None, description=\"Le sujet lié à la question. Les valeurs possibles sont 'Mathématiques', 'Informatique' ou None.\"\n",
    "    )\n",
    "    questions_prefix: Optional[str] = Field(\n",
    "        None, description=\"Le préfixe de la question (par exemple, 10, 1a., etc.). Autorise None.\"\n",
    "    )\n",
    "    \n",
    "output_parser = PydanticOutputParser(pydantic_object=Question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Question into Structured Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StructuredQuestion(llm, question: str, output_parser=output_parser, max_retries=10): \n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                Tu es un agent qui structure les données sous format JSON. Structure moi la question donnée si possible avec ces champs :\n",
    "                - exam : Le nom de l'épreuve (par exemple, MATHEMATIQUES A) sans accent.\n",
    "                - type : Le type de l'examen (écrit ou oral).\n",
    "                - contest : Le concours (par exemple, X-ENS).\n",
    "                - year : L'année de la question (par exemple, 2022).\n",
    "                - levels : Les niveaux ou préfixes de la question (par exemple, MP, PC).\n",
    "                - subjects : Le sujet lié à la question (par exemple, Mathématiques). \n",
    "                - questions_prefix : Le préfixe de la question (par exemple, 1a, 2a.).\n",
    "\n",
    "                Contrainte:\n",
    "                - Pour chaque champ, retourne une chaine de charactères, pas de liste !\n",
    "                \n",
    "                Encadre le résultat avec des balises `json`.\\n{format_instructions}\n",
    "                \"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Voici la question: {question}\"\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(format_instructions=output_parser.get_format_instructions())\n",
    "    \n",
    "    chain = prompt | llm | output_parser\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            res = chain.invoke({\"question\": question})\n",
    "\n",
    "            if res and any(value is not None for value in res.model_dump().values()):\n",
    "                if res.type is None:\n",
    "                    res.exam = None\n",
    "                    res.questions_prefix = []\n",
    "                return res \n",
    "\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed: All fields are None.\")\n",
    "            if attempt < max_retries - 1:\n",
    "                sleep(1)  # small delay between retries\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                sleep(1) \n",
    "                \n",
    "    print(\"All retry attempts failed.\")\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the Question Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Peux-tu me dire ce que dit le rapport de jury pour l'épreuve écrite de maths A pour la question 12 en filière MP pour le concours Polytechnique en 2022 ?\"\n",
    "\n",
    "# question_metadata = StructuredQuestion(llm, question, output_parser)\n",
    "\n",
    "# question_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-question Generation to Improve Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubQuestions(BaseModel):\n",
    "    question_one: str = Field(..., description=\"La première sous-question générée à partir de la question principale.\")\n",
    "    question_two: str = Field(..., description=\"La deuxième sous-question générée à partir de la question principale.\")\n",
    "    question_three: str = Field(..., description=\"La troisième sous-question générée à partir de la question principale.\")    \n",
    "    \n",
    "output_parser_sub_questions = PydanticOutputParser(pydantic_object=SubQuestions)\n",
    "\n",
    "def SubQuestionsGeneration(llm, question: str, question_metadata: Question, output_parser_sub_questions=output_parser_sub_questions): \n",
    "    metadata = (\n",
    "        f\"Examen : {question_metadata.exam or 'Non spécifié'}\\n\"\n",
    "        f\"Type : {question_metadata.type or 'Non spécifié'}\\n\"\n",
    "        f\"Concours : {question_metadata.contest or 'Non spécifié'}\\n\"\n",
    "        f\"Année : {question_metadata.year or 'Non spécifiée'}\\n\"\n",
    "        f\"Niveau : {question_metadata.levels or 'Non spécifié'}\\n\"\n",
    "        f\"Sujet : {question_metadata.subjects or 'Non spécifié'}\\n\"\n",
    "        f\"Préfixe de la question : {question_metadata.questions_prefix or 'Non spécifié'}\"\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                Ta tâche est de générer trois variantes de la question posée par l'utilisateur et des métadonnées suivantes:\n",
    "                {metadata}\n",
    "                \n",
    "                Ces reformulations doivent permettre de récupérer efficacement des documents pertinents dans une base de données vectorielle tout en compensant les limites de la recherche par similarité.\n",
    "\n",
    "                Le but est de générer des questions pertinentes en lien avec les rapports de jury \n",
    "                des sujets de concours des classes préparatoires scientifiques.\n",
    "                \n",
    "                Retourne le résultat sous forme de JSON avec les champs suivants :\n",
    "                - question_one : La première sous-question.\n",
    "                - question_two : La deuxième sous-question.\n",
    "                - question_three : La troisième sous-question.\n",
    "                \n",
    "                Encadre la réponse avec des balises `json`.\\n{format_instructions}\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Voici la question: {question}\"\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(format_instructions=output_parser_sub_questions.get_format_instructions())\n",
    "    \n",
    "    chain = prompt | llm | output_parser_sub_questions\n",
    "                \n",
    "    res = chain.invoke({\"question\": question, \"metadata\": metadata})\n",
    "        \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Sub-question Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subquestions = SubQuestionsGeneration(llm, question, question_metadata)\n",
    "\n",
    "# subquestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔙 Step-Back Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepBackQuestion(BaseModel):\n",
    "    step_back_question: str = Field(..., description=\"Une question plus générale générée à partir de la question principale.\")\n",
    "    \n",
    "output_parser_step_back = PydanticOutputParser(pydantic_object=StepBackQuestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def StepBackQuestionGeneration(llm, question: str, output_parser_step_back=output_parser_step_back):     \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                Ta tâche est de générer une question plus générale par rapport à la question donnée par l'utilisateur.\n",
    "                \n",
    "                La question générée doit fournir un contexte plus large à la question initiale tout en restant pertinente pour les rapports de jury des sujets de concours des classes préparatoires scientifiques.\n",
    "                \n",
    "                Encadre la réponse avec des balises `json`.\\n{format_instructions}\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Voici la question: {question}\"\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(format_instructions=output_parser_step_back.get_format_instructions())\n",
    "    \n",
    "    chain = prompt | llm | output_parser_step_back\n",
    "                \n",
    "    res = chain.invoke({\"question\": question})\n",
    "        \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the step back question parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_back = StepBackQuestionGeneration(llm, question, question_metadata)\n",
    "\n",
    "# step_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reciprocal Rank Fusion for Multi-query Retrieval\n",
    "\n",
    "Link to the method : https://medium.com/@devalshah1619/mathematical-intuition-behind-reciprocal-rank-fusion-rrf-explained-in-2-mins-002df0cc5e2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):            \n",
    "            page_content = doc[0].page_content\n",
    "            if page_content not in fused_scores:\n",
    "                fused_scores[page_content] = 0\n",
    "            fused_scores[page_content] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (page_content, score)\n",
    "        for page_content, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Generator for Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRAG(question_metadata):\n",
    "    filter = {}\n",
    "\n",
    "    for key, value in question_metadata.model_dump().items():\n",
    "        if value is None:\n",
    "            continue\n",
    "\n",
    "        # if key == \"questions_prefix\":\n",
    "        #     filter[key] = {\"$in\": [value]}\n",
    "        if key == \"levels\" or key == \"questions_prefix\":\n",
    "            filter[key] = {\"$ilike\": f\"%{value}%\"}\n",
    "        else:\n",
    "            filter[key] = {\"$eq\": value}\n",
    "    return filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RapportsJurysRAG:\n",
    "    def __init__(self, vector_store, question):\n",
    "        self.llm = OllamaLLM(model=\"llama3.1\")\n",
    "        self.vector_store = vector_store\n",
    "        self.question = question\n",
    "        self.multiQuerySubQuestions = False # add variants questions\n",
    "        self.QueryStepBack = False # add an overview context\n",
    "        self.multiQueryRAGFusion = False # use rrf ranking\n",
    "        self.nbDocs = 5 # nb docs in the context\n",
    "        self.similarity_score_threshold = 0.3 # 0: no similarity, 1: full similarity\n",
    "        \n",
    "    def retriever(self):\n",
    "        \n",
    "        try:\n",
    "            question_metadata = StructuredQuestion(self.llm, self.question)\n",
    "                                    \n",
    "            if (self.multiQuerySubQuestions):\n",
    "                \n",
    "                sub_questions_generation = SubQuestionsGeneration(self.llm, self.question, question_metadata)\n",
    "                sub_questions = dict(sub_questions_generation).values()\n",
    "                results = []\n",
    "                                \n",
    "                for sub_question in sub_questions:\n",
    "                    \n",
    "                    sub_question_metadata = StructuredQuestion(self.llm, sub_question)\n",
    "                    filter = filterRAG(sub_question_metadata)\n",
    "                    results.append(self.vector_store.similarity_search_with_relevance_scores(sub_question, k=self.nbDocs, filter=filter))\n",
    "                                    \n",
    "                if (self.multiQueryRAGFusion):\n",
    "                \n",
    "                    retrieval_results = reciprocal_rank_fusion(results)\n",
    "                    contexte = \"\\n\".join([page_content for page_content, _ in retrieval_results[:self.nbDocs]])  \n",
    "                    return contexte, question_metadata\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    flat_results = list(chain.from_iterable(results))\n",
    "                    unique_results = {doc[0].metadata['id']: doc for doc in flat_results}\n",
    "                    sorted_results = sorted(unique_results.values(), key=lambda x: x[1], reverse=True)\n",
    "                    contexte = \"\\n\".join([doc[0].page_content for doc in sorted_results[:self.nbDocs]])                      \n",
    "                    return contexte, question_metadata\n",
    "                \n",
    "            elif (self.QueryStepBack):\n",
    "                \n",
    "                step_back_question_output = StepBackQuestionGeneration(self.llm, self.question)   \n",
    "                step_back_question = step_back_question_output.step_back_question            \n",
    "                step_back_question_metadata = StructuredQuestion(self.llm, step_back_question)                   \n",
    "                filter_question = filterRAG(question_metadata)\n",
    "                filter_step_back = filterRAG(step_back_question_metadata)\n",
    "                results_question = self.vector_store.similarity_search_with_relevance_scores(self.question, k=5, filter=filter_question)\n",
    "                results_step_back_question = self.vector_store.similarity_search_with_relevance_scores(step_back_question, k=5, filter=filter_step_back)                \n",
    "                contexte = \"\\n\".join([result[0].page_content for result in results_question[:self.nbDocs] + results_step_back_question[:self.nbDocs]])\n",
    "                return contexte, question_metadata\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                filter = filterRAG(question_metadata)\n",
    "                results = self.vector_store.similarity_search_with_relevance_scores(self.question, k=self.nbDocs, filter=filter) #, score_threshold=self.similarity_score_threshold\n",
    "                contexte = \"\\n\".join([result[0].page_content for result in results])  \n",
    "                return contexte, question_metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return \"\", {}\n",
    "        \n",
    "        \n",
    "    def create_chain(self):\n",
    "        template = \"\"\"\n",
    "        Tu es un agent IA sur les rapports de jury concernant les sujets de concours en classe préparatoire scientifique. \n",
    "        Réponds à la question de l'utilisateur en utilisant le contexte. La metadata est censé t'aider à discerner les parties pertinentes du contexte pour générer la réponse. \n",
    "        Si tu ne connais pas la réponse, dis simplement que tu ne sais pas.\n",
    "        Question: {question} \n",
    "        Metadata: {metadata}\n",
    "        Contexte: {contexte} \n",
    "        \"\"\" \n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "            \n",
    "        chain = (\n",
    "             prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        return chain\n",
    "\n",
    "    def answer(self):\n",
    "        contexte, question_metadata = self.retriever()\n",
    "        \n",
    "        input_data = {\n",
    "            \"question\": self.question,\n",
    "            \"metadata\": question_metadata,\n",
    "            \"contexte\": contexte\n",
    "        }\n",
    "        \n",
    "        chain = self.create_chain()\n",
    "\n",
    "        res = chain.invoke(input=input_data)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Peux-tu me dire ce que dit le rapport de jury pour l'épreuve écrite de maths A pour la question 12 en filière MP pour le concours Polytechnique en 2022 ?\"\n",
    "\n",
    "rapports_jurys = RapportsJurysRAG(vector_store, question)\n",
    "\n",
    "answer = rapports_jurys.answer()\n",
    "\n",
    "print(\"Réponse: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapportsjurys-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
